Personal note for general machine learning techniques.

1. Identify the problem -- Supervised		== regression
											== classification
												
						-- Unsupervised		== clustering
						-- Half supervised  
2. Amount of data  -- > 10G ---> Distributed machine learning (hardoop,hive...)
					  < 10G ---> Single machine 

3. Feature engineering
	3.1 Know the data--EDA(Exploratory Data Analysis)
	3.2 Remove -- const
			   -- Correlated 
			   -- Unrelated (Id, name)
			   -- Outlier === Grubb's test
						  === Dixon's Q test
						  === Chauvenet's Criterion
						  === Piere's Criterion
	3.3 Pre-process 
			-- Numerical  === Standarization: scale to a u=0, sigma=1 standard distribution
						  === Normalize: Normalize value to [0-1]
						  === Re-sample
						  (need: regression,KNN,SVM,NN,cluster)
						  (don't need: tree based, Bayes)
			-- Categorical  === One hot encoding

	3.4 Create new features
	3.5 Missing value 

4. Feature selection
	4.1 Filter
	4.2 Wrapper
	4.3 Embedded  ????
	4.4 Decomposition -- PCA (Principal component analysis)
					  -- LDA

4.5 评价函数
	RMSE: More sensitive to outliers, if not much outlier data use this.  
	MAE: Less sensitive to outliers, if more outlier data use this.

5. Model selection
	5.1 Regression
		5.1.1 Linear regression
			median regression (robust)
			5.1.1.1 LASSO regression (regularization) add punishment factor to reduce over fitting
			5.1.1.2 Ridge regression  
			Note: LASSO can set weight to 0, can use as feature selection.Good for interpretability.
			  Ridge set weight to a small number. 
		5.2.2 K-nearest-neighbors
		5.2.3 Bayesian
	
	5.2 Classification
		5.2.1 Logistic regression
		5.2.2 SVM
		5.2.3 Decision Tree
		5.2.4 K-nearest-neighbors
		5.2.5 Bayesian
		
	5.3 Clustering
		K-mean
		Affinity Propagation
		Hierarchical 
		DBSCAN

	5.4 Tree based
		5.4.1 Decision tree  --  works for reguression and classification
		5.4.2 Ramdom Rorest	 --  
		5.4.3 Gradient Boost Trees  --  
		
		Compare:
		+-------------------------------+-----------------------------------+
		|		Random Forest			|		Gradient Boost				|
		+-------------------------------+-----------------------------------+
		|			Bagging				|			Boosting				|
		+-------------------------------+-----------------------------------+
		|  Big trees to lower bias		|  Small trees to keep variance low	|
		+-------------------------------+-----------------------------------+
		|  More trees to lower variance	|  More trees to lower bias			|
		+-------------------------------+-----------------------------------+
		|		Fast, Easy tuning		|  Overfitting, slow, hard to train	|
		|			Less overfit		|			Better Result			|
		+-------------------------------+-----------------------------------+

		Stacking: Use result from different models.
				  Use logistic regression to blend them.
	
	5.5 Neural Network
		5.5.1 ANN  --  
		5.5.2 CNN  --  Convolutional NN  ==  针对图像处理
		5.5.3 RNN  --  recurent NN	 ==  时间递归
				   --  recursive NN  ==  结构递归
			LSTM
			Seq2Seq
		




6. Train, test and validation
	Model regularization (constrain the model to prevent overfitting)
	Cross validation
	What is hyperparameter??

7, Tuning






7. Result evaluation
	p-value, T-test, z-test, z scoring, Kurtosis, Central Limit theorem, IQR, 

	Chi square 
	confusion matrix  
				+-----------------+-----------------+
				| True Positives  | False Negatives |
				+-----------------+-----------------+
				| False Positives | True Negatives  |
				+-----------------+-----------------+
	ROC  /  AUC 
8. What's after
	Deployment
	New data: incremental learning.(partial_fit)



Interview questions:

Imbalanced data:
1. Up-sample Minority Class
	from sklearn.utils import resample
	minority = df[df['target']==rare_target]
	majority = df[df['target']==most_target]
	minority_upsampled = resample(minority,
								  replace=True,
								  n_samples=len(majority),
								  random_state=2019)
	df_upsampled = pd.concat([majority,minority_upsampled])
2. Down-sample Majority Class
	majority = df[df['target']==most_target]
	minority = df[df['target']==rare_target]
	majority_downsampled = resample(majority,
								    replace=False,
									n_samples=len(minority),
									random_state=2019)
	df_downsampled = pd.concat([majority_downsampled, minority])
3. Change performance metric
	Area Under ROC Curve
	from sklearn.metrics import roc_auc_score
	prob_y = model.predict_proba(X)
	prob_y = [p[1] for p in prob_y]  # Keep only the positive class?
	print(roc_auc_score(y,prob_y))
From paper "Using random forest to learn imbalanced data".
1. Cost sensitive learning: High cost for misclassification of minority class.
2. Sample approch.



Decision tree split standard (shang) sklearn parameter criterion='gini' / 'entropy' to evaluate homogeneity
missing value
multicorrolation features
correlated features: 1. PCA, 2. Lasso Regularision
type1 type2 error    type1 FP  type2 FN

b value
standarize (x-mean)/sigma turn data into normal distribution
normalize:  (x-xmin)/(xmax-xmin) turn data into [0-1]

k mean: Use elbow method to determin K (plot k-Sum(D) curve and find "elbow")
p value
chi square

