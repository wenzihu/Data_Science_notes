sklearn learning note

0. Splitting the data into trainning and test
	from sklearn.cross_validation import train_test_split
	X_train,X_test,Y_train,Y_test = 
		train_test_split(X,y,test_size=0.25,random_state=33)


1. Pre-processing
	预处理库： from sklearn import preprocessing 
	
	Standardization(z-score normalization) 标准化
		公式：z=(x-mean)/std
		实现：简单方法：x = sklearn.preprocessing.scale(x)
			  库的方法：scaler = preprocessing.StandardScaler().fit(x)
						scaled_x = scaler.transform(x)

		应用：k-nearest neighbor,k-mean,logistic regression,SVM,Neural Network,etc
	
	Min-Max scaling 
		公式：xNorm = (x-xmin)/(xmax-xmin) 把x规范到0-1范围
		实现：简单方法：x = sklearn.preprocessing.minmax_scale(x)
			  库的方法：scaler = sklearn.preprocessing.MinMaxScaler().fit(x)
						scaled_x = scaler.transform(x)
	MaxAbs scaling
		公式：
		实现：简单方法：x = preprocessing.maxabs_scale(x)
			  库的方法：scaler = preprocessing.MaxAbsScaler().fit(x)
						scaled_x = scaler.transform(x)
	
	Binarize 二值化
		设定阈值将给定数据二值化为0，1
		实现：简单方法：x = perprocessing.binarize(x,threshold = ??)
			  库的方法：scaler = preprocessing.Binarizer(threshold=??)
						scaled_x = scaler.transform(x)

	normalization 正则华
		公式：
		实现：简单方法：x_norm = preprocessing.normalize(x)
			  库的方法：normalizer = preprocessing.Normalizer().fit(x)
						x_norm = normalizer.transform(x)

	数据分割：
		from sklearn.cross_validation import train_test_split
		x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3)

	One-hot-encoding
		??????

2. Feature selection
	from sklearn import feature_selection
	2.1 filter
	2.2 wrapper
	2.3 embedded
		??regularization l0
						 l1: lasso
						 l2: ridge

3. Models 
	LinearRegression(线性回归)：model = sklearn.linear_model.LinearRegression()
	RidgeRegression(岭回归)：  model = sklearn.linear_model.Ridge(alpha=?)
	Bayes(贝叶斯-高斯)：model = sklearn.naive_bayes.GassianNB()
			  多项式：  model = sklearn.naive_bayes.MultinomialNB()
	DecisionTree(决策树分类)： model = sklearn.tree.DecisionTreeClassifier()
			    (决策树回归)： model = sklearn.tree.DecisionTreeRegressor()
	RandomForest(随机森林):model = sklearn.ensemble.RandomForestClassifier(n_estimators=??)
			           主要参数:n_estimators    
	GradientBoosting: model = sklearn.ensemble.GradientBoostingClassifier()
	SVM(支持向量机)： model = sklearn.svm.SVC()
	K-mean(聚类):model = sklearn.cluster.KMeans(n_clusters=?, random_state=0).fit(x)
	KNN:		   model = sklearn.neighbors.KNeighborsClassifier()
					       主要参数: K neighbors 数量

	Model Optimization(优化模型)
	参数选择: grid search  
		from sklearn.grid_search import GridSearchCV
		param_grid = {'Para1':[0.1,1,10,100,1000], 'Para2':[1,0.1,0.01,0.001]}
		grid = GridSearchCV(SVC(), param_grid, verbose=3)
		grid.fit(X_train,y_train) 训练模型，找到最好的参数后自动将最佳参数给模型
		之后的预测和普通模型一样
		grid.best_params_ , grid_best_estimator_ 返回计算得到的最佳参数

4. Process 通用流程
	加载数据：从文件或网页读入数据，多为DataFrame格式
	整理数据：留下有用的feature，整理为X--n by m，y--n by 1 (n:样本数，m:特征数)
	预处理：  标准化数据，拆分为训练组与测试组
	降维：？？？？？
	选择模型
	训练模型：model.fit(X_train,y_train)
	预测：y_pred = model.predict(X_test)
	验证：from sklearn.model_selection import cross_validate
	查看结果：from sklearn.metrics import classification_report, confusion_matrix
	print(confusion_matrix(y_test, y_pred))
	print(classification_report(y_test,y_pred))

	Pipeline: from sklearn.pipeline import Pipeline
	pipeline = Pipeline([])
